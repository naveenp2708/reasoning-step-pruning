{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Reasoning Step Pruning via Attention Scores\n",
        "\n",
        "**Based on:**\n",
        "1. [Think Clearly: Improving Reasoning via Redundant Token Pruning](https://arxiv.org/abs/2507.08806)\n",
        "2. [TRAAC: Think Right with Adaptive, Attentive Compression](https://arxiv.org/abs/2510.01581)\n",
        "\n",
        "**Author:** Naveen Pasupuleti"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": ["## 0. Setup"],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers accelerate bitsandbytes sentencepiece protobuf -q"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os, re, json, time, torch\n",
        "import numpy as np\n",
        "from typing import List, Dict, Tuple, Optional\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(f'GPU Available: {torch.cuda.is_available()}')\n",
        "if torch.cuda.is_available():\n",
        "    print(f'GPU: {torch.cuda.get_device_name(0)}')\n",
        "    props = torch.cuda.get_device_properties(0)\n",
        "    mem = getattr(props, 'total_memory', None) or getattr(props, 'total_mem', 0)\n",
        "    print(f'Memory: {mem / 1e9:.1f} GB')\n",
        "print(f'PyTorch: {torch.__version__}')"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": ["## 1. AIME24 Dataset"],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "AIME24_PROBLEMS = [\n",
        "    {\"id\": 1, \"problem\": \"Every morning Asha decides randomly whether to walk left or right and independently Sasha does the same. They start on different ends of a 4-block long street. What is the probability that they meet? Express your answer as a fraction m/n in lowest terms and find m+n.\", \"answer\": 31},\n",
        "    {\"id\": 2, \"problem\": \"There exist real numbers x and y, both greater than 1, such that log_x(y^x) = log_y(x^(4y)) = 10. Find xy.\", \"answer\": 25},\n",
        "    {\"id\": 3, \"problem\": \"Alice and Bob play a game. Alice starts first and they alternate turns. Alice's move is to choose an integer from 1 to 6 (inclusive) and add it to the running total. Bob does the same. The player who brings the running total to exactly 2024 wins. What is the smallest starting move Alice can use to guarantee a win?\", \"answer\": 5},\n",
        "    {\"id\": 4, \"problem\": \"Let x, y, and z be positive real numbers satisfying the system: log_2(x/yz) = 1/2, log_2(y/xz) = 1/3, log_2(z/xy) = 1/4. Find the value of |log_2(x^4 y^3 z^2)|. Express as a fraction p/q in lowest terms and find p+q.\", \"answer\": 33},\n",
        "    {\"id\": 5, \"problem\": \"Rectangle ABCD has side lengths AB=10 and BC=4. Point M is the midpoint of CD. Triangle AMB is removed, leaving a quadrilateral. Find the perimeter of the quadrilateral formed. If the answer is a+b*sqrt(c), find a+b+c.\", \"answer\": 18},\n",
        "]\n",
        "\n",
        "print(f'Loaded {len(AIME24_PROBLEMS)} AIME24 problems')"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": ["## 2. Load Quantized Reasoning Model"],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_NAME = 'deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B'\n",
        "\n",
        "print(f'Loading {MODEL_NAME}...')\n",
        "print('This may take 2-3 minutes for download + quantization.')\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type='nf4',\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        ")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map='auto',\n",
        "    trust_remote_code=True,\n",
        "    attn_implementation='eager',\n",
        ")\n",
        "model.eval()\n",
        "\n",
        "print('Model loaded successfully!')\n",
        "if torch.cuda.is_available():\n",
        "    alloc = torch.cuda.memory_allocated() / 1e9\n",
        "    print(f'GPU Memory used: {alloc:.1f} GB')"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": ["## 3. Core Functions"],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_reasoning(prompt, max_tokens=4096):\n",
        "    messages = [{'role': 'user', 'content': prompt}]\n",
        "    if hasattr(tokenizer, 'apply_chat_template'):\n",
        "        text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "    else:\n",
        "        text = f'<|im_start|>user\\n{prompt}<|im_end|>\\n<|im_start|>assistant\\n'\n",
        "    inputs = tokenizer(text, return_tensors='pt').to(model.device)\n",
        "    input_len = inputs['input_ids'].shape[1]\n",
        "    with torch.no_grad():\n",
        "        out = model.generate(**inputs, max_new_tokens=max_tokens, temperature=0.6, top_p=0.95, do_sample=True)\n",
        "    return tokenizer.decode(out[0][input_len:], skip_special_tokens=True)\n",
        "\n",
        "\n",
        "def segment_steps(text):\n",
        "    think_match = re.search(r'<think>(.*?)</think>', text, re.DOTALL)\n",
        "    if think_match:\n",
        "        thinking = think_match.group(1)\n",
        "        answer = text[think_match.end():]\n",
        "    else:\n",
        "        thinking = text\n",
        "        answer = ''\n",
        "    chunks = re.split(r'\\n\\n+', thinking)\n",
        "    steps = []\n",
        "    for chunk in chunks:\n",
        "        chunk = chunk.strip()\n",
        "        if len(chunk) >= 10:\n",
        "            steps.append({'text': chunk, 'step_id': len(steps)})\n",
        "    return steps, thinking, answer\n",
        "\n",
        "\n",
        "def score_steps_by_attention(steps, full_text, prompt):\n",
        "    messages = [{'role': 'user', 'content': prompt}, {'role': 'assistant', 'content': full_text}]\n",
        "    if hasattr(tokenizer, 'apply_chat_template'):\n",
        "        text = tokenizer.apply_chat_template(messages, tokenize=False)\n",
        "    else:\n",
        "        text = f'<|im_start|>user\\n{prompt}<|im_end|>\\n<|im_start|>assistant\\n{full_text}<|im_end|>'\n",
        "    inputs = tokenizer(text, return_tensors='pt', truncation=True, max_length=4096).to(model.device)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs, output_attentions=True)\n",
        "    last_attn = torch.stack([layer[:, :, -1, :] for layer in outputs.attentions]).mean(dim=(0, 2))[0].cpu().numpy()\n",
        "    for marker in ['<|im_start|>assistant', 'assistant\\n', '<|assistant|>']:\n",
        "        marker_pos = text.find(marker)\n",
        "        if marker_pos != -1:\n",
        "            offset = len(tokenizer.encode(text[:marker_pos + len(marker) + 1]))\n",
        "            break\n",
        "    else:\n",
        "        offset = 0\n",
        "    for step in steps:\n",
        "        start = full_text.find(step['text'])\n",
        "        if start == -1:\n",
        "            step['importance'] = 0.0\n",
        "            step['avg_attention'] = 0.0\n",
        "            step['num_tokens'] = 0\n",
        "            continue\n",
        "        prefix_toks = len(tokenizer.encode(full_text[:start], add_special_tokens=False))\n",
        "        step_toks = len(tokenizer.encode(step['text'], add_special_tokens=False))\n",
        "        t_start = max(0, min(offset + prefix_toks, len(last_attn) - 1))\n",
        "        t_end = max(t_start + 1, min(offset + prefix_toks + step_toks, len(last_attn)))\n",
        "        step_attn = last_attn[t_start:t_end]\n",
        "        step['importance'] = float(np.sum(step_attn))\n",
        "        step['avg_attention'] = float(np.mean(step_attn)) if len(step_attn) > 0 else 0.0\n",
        "        step['num_tokens'] = step_toks\n",
        "    total = sum(s['importance'] for s in steps)\n",
        "    if total > 0:\n",
        "        for s in steps:\n",
        "            s['importance'] /= total\n",
        "    del outputs\n",
        "    torch.cuda.empty_cache()\n",
        "    return steps\n",
        "\n",
        "\n",
        "def prune_steps(steps, thinking, answer, threshold):\n",
        "    if threshold == 0.0 or not steps:\n",
        "        return thinking + '\\n' + answer, len(steps), len(steps)\n",
        "    importances = [s['importance'] for s in steps]\n",
        "    cutoff = np.percentile(importances, threshold * 100)\n",
        "    kept = [s for s in steps if s['importance'] >= cutoff]\n",
        "    if not kept:\n",
        "        kept = [max(steps, key=lambda s: s['importance'])]\n",
        "    pruned_text = '\\n\\n'.join(s['text'] for s in kept) + '\\n' + answer\n",
        "    return pruned_text, len(steps), len(kept)\n",
        "\n",
        "\n",
        "def extract_answer(text):\n",
        "    for pattern in [r'\\\\boxed\\{(\\d+)\\}', r'the answer is\\s*[:\\s]*(\\d+)',\n",
        "                     r'final answer[:\\s]*(\\d+)', r'answer[:\\s]*\\*?\\*?(\\d+)', r'= (\\d+)\\s*$']:\n",
        "        matches = re.findall(pattern, text, re.IGNORECASE | re.MULTILINE)\n",
        "        if matches:\n",
        "            try:\n",
        "                ans = int(matches[-1])\n",
        "                if 0 <= ans <= 999:\n",
        "                    return ans\n",
        "            except ValueError:\n",
        "                continue\n",
        "    numbers = re.findall(r'\\b(\\d{1,3})\\b', text[-300:])\n",
        "    return int(numbers[-1]) if numbers else None\n",
        "\n",
        "\n",
        "def reevaluate(problem, pruned_reasoning):\n",
        "    prompt = (f'Here is a math problem and a partial reasoning trace. '\n",
        "              f'Based on the reasoning, give the final numeric answer.\\n\\n'\n",
        "              f'Problem: {problem}\\n\\nReasoning:\\n{pruned_reasoning}\\n\\n'\n",
        "              f'Based on the above reasoning, the final answer is:')\n",
        "    return generate_reasoning(prompt, max_tokens=512)\n",
        "\n",
        "\n",
        "print('All functions defined.')"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": ["## 4. Run Experiment"],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "THRESHOLDS = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5]\n",
        "metrics = {t: {'correct': 0, 'total': 0, 'kept_pct': [], 'lengths': []} for t in THRESHOLDS}\n",
        "all_step_data = []\n",
        "\n",
        "print(f'Running: {len(AIME24_PROBLEMS)} problems x {len(THRESHOLDS)} thresholds')\n",
        "print(f'Estimated time: 15-25 minutes\\n')\n",
        "\n",
        "for i, prob in enumerate(AIME24_PROBLEMS):\n",
        "    print(f'\\n{\"=\"*60}')\n",
        "    print(f'Problem {i+1}/{len(AIME24_PROBLEMS)} (ID: {prob[\"id\"]})')\n",
        "    print(f'{\"=\"*60}')\n",
        "\n",
        "    t0 = time.time()\n",
        "    full_text = generate_reasoning(prob['problem'])\n",
        "    print(f'  Generated {len(full_text)} chars in {time.time()-t0:.1f}s')\n",
        "\n",
        "    steps, thinking, answer = segment_steps(full_text)\n",
        "    print(f'  {len(steps)} reasoning steps')\n",
        "\n",
        "    if not steps:\n",
        "        print('  No steps found, skipping')\n",
        "        for t in THRESHOLDS:\n",
        "            metrics[t]['total'] += 1\n",
        "        continue\n",
        "\n",
        "    print('  Scoring by attention...')\n",
        "    try:\n",
        "        steps = score_steps_by_attention(steps, full_text, prob['problem'])\n",
        "        sorted_s = sorted(steps, key=lambda s: s['importance'], reverse=True)\n",
        "        print(f'  Top: score={sorted_s[0][\"importance\"]:.4f}')\n",
        "        print(f'  Bottom: score={sorted_s[-1][\"importance\"]:.4f}')\n",
        "    except Exception as e:\n",
        "        print(f'  Attention failed: {e}, using uniform')\n",
        "        for s in steps:\n",
        "            s['importance'] = 1.0 / len(steps)\n",
        "            s['avg_attention'] = 1.0 / len(steps)\n",
        "\n",
        "    all_step_data.append({'problem_id': prob['id'], 'steps': steps})\n",
        "\n",
        "    for t in THRESHOLDS:\n",
        "        pruned, orig, kept = prune_steps(steps, thinking, answer, t)\n",
        "        if t == 0.0:\n",
        "            predicted = extract_answer(full_text)\n",
        "        else:\n",
        "            eval_out = reevaluate(prob['problem'], pruned)\n",
        "            predicted = extract_answer(eval_out)\n",
        "            if predicted is None:\n",
        "                predicted = extract_answer(pruned)\n",
        "        correct = predicted is not None and predicted == prob['answer']\n",
        "        metrics[t]['correct'] += int(correct)\n",
        "        metrics[t]['total'] += 1\n",
        "        metrics[t]['kept_pct'].append(kept / max(orig, 1))\n",
        "        metrics[t]['lengths'].append(len(pruned))\n",
        "        mark = 'Y' if correct else 'N'\n",
        "        print(f'  t={t:.1f}: {orig}->{kept} steps, pred={predicted}, exp={prob[\"answer\"]}, {mark}')\n",
        "\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "print(f'\\nDone!')"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": ["## 5. Comparison Table"],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "print()\n",
        "print('=' * 85)\n",
        "print('  COMPARISON TABLE: Attention-Based Reasoning Pruning on AIME24')\n",
        "print(f'  Model: {MODEL_NAME}')\n",
        "print(f'  Papers: Think Clearly (2507.08806) + TRAAC (2510.01581)')\n",
        "print('=' * 85)\n",
        "print(f'{\"Threshold\":<14} {\"Accuracy\":<14} {\"Steps Kept\":<14} {\"Avg Length\":<16} {\"Correct/Total\":<15}')\n",
        "print('-' * 85)\n",
        "\n",
        "baseline_acc = None\n",
        "baseline_len = None\n",
        "\n",
        "for t in THRESHOLDS:\n",
        "    m = metrics[t]\n",
        "    if m['total'] == 0:\n",
        "        continue\n",
        "    acc = m['correct'] / m['total'] * 100\n",
        "    kept = np.mean(m['kept_pct']) * 100 if m['kept_pct'] else 0\n",
        "    avg_len = np.mean(m['lengths']) if m['lengths'] else 0\n",
        "    if t == 0.0:\n",
        "        baseline_acc = acc\n",
        "        baseline_len = avg_len\n",
        "        label = f'{t:.1f} (baseline)'\n",
        "        delta_acc = ''\n",
        "        delta_len = ''\n",
        "    else:\n",
        "        label = f'{t:.1f}'\n",
        "        delta_acc = f' ({acc - baseline_acc:+.1f}%)' if baseline_acc is not None else ''\n",
        "        delta_len = f' (-{(1 - avg_len/baseline_len)*100:.0f}%)' if baseline_len and baseline_len > 0 else ''\n",
        "    print(f'{label:<14} {acc:.1f}%{delta_acc:<10} {kept:.1f}%{\"\":<10} {avg_len:.0f}{delta_len:<12} {m[\"correct\"]}/{m[\"total\"]}')\n",
        "\n",
        "print('=' * 85)\n",
        "print()\n",
        "print('Interpretation:')\n",
        "print('  - Threshold 0.0 = full chain (baseline, no pruning)')\n",
        "print('  - Moderate pruning (0.1-0.3): removes distracting steps, may improve accuracy')\n",
        "print('  - Aggressive pruning (0.4+): risks removing critical steps')"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": ["## 6. Visualizations"],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Attention importance per step\n",
        "if all_step_data:\n",
        "    n = len(all_step_data)\n",
        "    fig, axes = plt.subplots(n, 1, figsize=(12, 3 * n))\n",
        "    if n == 1:\n",
        "        axes = [axes]\n",
        "    for idx, data in enumerate(all_step_data):\n",
        "        ax = axes[idx]\n",
        "        steps = data['steps']\n",
        "        ids = [s['step_id'] for s in steps]\n",
        "        imps = [s['importance'] for s in steps]\n",
        "        if not imps:\n",
        "            continue\n",
        "        med = np.median(imps)\n",
        "        colors = ['#27ae60' if v >= med else '#e74c3c' for v in imps]\n",
        "        ax.bar(ids, imps, color=colors, alpha=0.85)\n",
        "        ax.axhline(y=med, color='black', linestyle='--', alpha=0.4, label=f'Median')\n",
        "        ax.set_xlabel('Step ID')\n",
        "        ax.set_ylabel('Importance')\n",
        "        ax.set_title(f'Problem {data[\"problem_id\"]}: Step Importance (Green=Keep, Red=Prune)')\n",
        "        ax.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('attention_importance.png', dpi=150, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "# Accuracy vs threshold\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
        "ts, accs, kepts = [], [], []\n",
        "for t in THRESHOLDS:\n",
        "    m = metrics[t]\n",
        "    if m['total'] > 0:\n",
        "        ts.append(t)\n",
        "        accs.append(m['correct'] / m['total'] * 100)\n",
        "        kepts.append(np.mean(m['kept_pct']) * 100)\n",
        "\n",
        "ax1.plot(ts, accs, 'bo-', linewidth=2, markersize=8)\n",
        "ax1.set_xlabel('Pruning Threshold')\n",
        "ax1.set_ylabel('Accuracy (%)')\n",
        "ax1.set_title('Accuracy vs Pruning')\n",
        "ax1.set_ylim([-5, 105])\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "ax2.plot(ts, kepts, 'rs-', linewidth=2, markersize=8)\n",
        "ax2.set_xlabel('Pruning Threshold')\n",
        "ax2.set_ylabel('Steps Kept (%)')\n",
        "ax2.set_title('Compression vs Pruning')\n",
        "ax2.set_ylim([-5, 105])\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "plt.suptitle('Attention-Based Reasoning Pruning - AIME24', fontsize=14, y=1.02)\n",
        "plt.tight_layout()\n",
        "plt.savefig('pruning_results.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "print('Plots saved.')"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": ["## 7. Save Results"],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "results = {\n",
        "    'model': MODEL_NAME,\n",
        "    'dataset': 'AIME24',\n",
        "    'num_problems': len(AIME24_PROBLEMS),\n",
        "    'papers': ['Think Clearly (arXiv:2507.08806)', 'TRAAC (arXiv:2510.01581)'],\n",
        "    'thresholds': {}\n",
        "}\n",
        "for t in THRESHOLDS:\n",
        "    m = metrics[t]\n",
        "    if m['total'] > 0:\n",
        "        results['thresholds'][str(t)] = {\n",
        "            'accuracy': round(m['correct'] / m['total'] * 100, 1),\n",
        "            'steps_kept_pct': round(float(np.mean(m['kept_pct'])) * 100, 1),\n",
        "            'avg_length': round(float(np.mean(m['lengths']))),\n",
        "            'correct': m['correct'],\n",
        "            'total': m['total'],\n",
        "        }\n",
        "\n",
        "with open('aime24_pruning_results.json', 'w') as f:\n",
        "    json.dump(results, f, indent=2)\n",
        "\n",
        "print('Saved: aime24_pruning_results.json')\n",
        "print()\n",
        "print('Upload to GitHub:')\n",
        "print('  1. This notebook')\n",
        "print('  2. aime24_pruning_results.json')\n",
        "print('  3. attention_importance.png')\n",
        "print('  4. pruning_results.png')\n",
        "print('  5. README.md')"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    }
  ]
}
