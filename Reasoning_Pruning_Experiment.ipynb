{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Reasoning Step Pruning via Attention Scores\n",
        "\n",
        "**Based on:**\n",
        "1. [Think Clearly: Improving Reasoning via Redundant Token Pruning](https://arxiv.org/abs/2507.08806)\n",
        "2. [TRAAC: Think Right with Adaptive, Attentive Compression](https://arxiv.org/abs/2510.01581)\n",
        "\n",
        "**Goal:** Prune reasoning steps in LLM chain-of-thought based on attention importance scores, and measure how it affects math problem-solving accuracy on AIME24.\n",
        "\n",
        "**Author:** Naveen Pasupuleti\n",
        "\n",
        "---\n",
        "\n",
        "### Key Idea\n",
        "\n",
        "Reasoning LLMs generate long chain-of-thought traces with significant redundancy. By analyzing attention patterns:\n",
        "- Steps that receive **high attention** from subsequent tokens are **critical** for the final answer\n",
        "- Steps that receive **low attention** are **redundant/distracting** and can be pruned\n",
        "- Moderate pruning can actually **improve** accuracy by reducing distraction (\"Think Clearly\" finding)\n",
        "- We experiment across multiple pruning thresholds to find the sweet spot"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 0. Setup"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers accelerate bitsandbytes torch sentencepiece protobuf datasets -q"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os, re, json, time, torch\n",
        "import numpy as np\n",
        "from typing import List, Dict, Tuple, Optional\n",
        "from dataclasses import dataclass, field\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(f'GPU Available: {torch.cuda.is_available()}')\n",
        "if torch.cuda.is_available():\n",
        "    print(f'GPU: {torch.cuda.get_device_name(0)}')\n",
        "    print(f'Memory: {torch.cuda.get_device_properties(0).total_mem / 1e9:.1f} GB')"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. AIME24 Dataset"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "AIME24_PROBLEMS = [\n",
        "    {\"id\": 1, \"problem\": \"Every morning Asha decides randomly whether to walk left or right and independently Sasha does the same. They start on different ends of a 4-block long street. What is the probability that they meet? Express your answer as a fraction m/n in lowest terms and find m+n.\", \"answer\": 31},\n",
        "    {\"id\": 2, \"problem\": \"There exist real numbers x and y, both greater than 1, such that log_x(y^x) = log_y(x^(4y)) = 10. Find xy.\", \"answer\": 25},\n",
        "    {\"id\": 3, \"problem\": \"Alice and Bob play a game. Alice starts first and they alternate turns. Alice's move is to choose an integer from 1 to 6 (inclusive) and add it to the running total. Bob does the same. The player who brings the running total to exactly 2024 wins. What is the smallest starting move Alice can use to guarantee a win?\", \"answer\": 5},\n",
        "    {\"id\": 4, \"problem\": \"Let x, y, and z be positive real numbers satisfying the system: log_2(x/yz) = 1/2, log_2(y/xz) = 1/3, log_2(z/xy) = 1/4. Find the value of |log_2(x^4 y^3 z^2)|. Express as a fraction p/q in lowest terms and find p+q.\", \"answer\": 33},\n",
        "    {\"id\": 5, \"problem\": \"Rectangle ABCD has side lengths AB=10 and BC=4. Point M is the midpoint of CD. Triangle AMB is removed, leaving a quadrilateral. Find the perimeter of the quadrilateral formed. If the answer is a+b*sqrt(c), find a+b+c.\", \"answer\": 18},\n",
        "]\n",
        "\n",
        "print(f'Loaded {len(AIME24_PROBLEMS)} AIME24 problems')"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Load Quantized Reasoning Model"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_NAME = 'deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B'\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type='nf4',\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        ")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map='auto',\n",
        "    trust_remote_code=True,\n",
        "    attn_implementation='eager',\n",
        ")\n",
        "model.eval()\n",
        "print('Model loaded!')"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Core Functions"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_reasoning(prompt, max_tokens=4096):\n",
        "    \"\"\"Generate a full reasoning chain.\"\"\"\n",
        "    messages = [{'role': 'user', 'content': prompt}]\n",
        "    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "    inputs = tokenizer(text, return_tensors='pt').to(model.device)\n",
        "    input_len = inputs['input_ids'].shape[1]\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        out = model.generate(**inputs, max_new_tokens=max_tokens, temperature=0.6, top_p=0.95, do_sample=True)\n",
        "    \n",
        "    return tokenizer.decode(out[0][input_len:], skip_special_tokens=True)\n",
        "\n",
        "\n",
        "def segment_steps(text):\n",
        "    \"\"\"Split reasoning into discrete steps.\"\"\"\n",
        "    think_match = re.search(r'<think>(.*?)</think>', text, re.DOTALL)\n",
        "    thinking = think_match.group(1) if think_match else text\n",
        "    answer = text[think_match.end():] if think_match else ''\n",
        "    \n",
        "    chunks = re.split(r'\\n\\n+', thinking)\n",
        "    steps = []\n",
        "    for chunk in chunks:\n",
        "        chunk = chunk.strip()\n",
        "        if len(chunk) >= 10:\n",
        "            steps.append({'text': chunk, 'step_id': len(steps)})\n",
        "    \n",
        "    return steps, thinking, answer\n",
        "\n",
        "\n",
        "def score_steps_by_attention(steps, full_text, prompt):\n",
        "    \"\"\"Score each step by how much attention the final tokens pay to it.\"\"\"\n",
        "    messages = [\n",
        "        {'role': 'user', 'content': prompt},\n",
        "        {'role': 'assistant', 'content': full_text}\n",
        "    ]\n",
        "    text = tokenizer.apply_chat_template(messages, tokenize=False)\n",
        "    inputs = tokenizer(text, return_tensors='pt', truncation=True, max_length=4096).to(model.device)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs, output_attentions=True)\n",
        "    \n",
        "    # Average attention from last token across all layers/heads\n",
        "    last_attn = torch.stack([l[:, :, -1, :] for l in outputs.attentions]).mean(dim=(0, 2))[0].cpu().numpy()\n",
        "    \n",
        "    # Find where assistant response starts\n",
        "    assistant_marker = '<|im_start|>assistant'\n",
        "    marker_pos = text.find(assistant_marker)\n",
        "    offset = len(tokenizer.encode(text[:marker_pos + len(assistant_marker) + 1])) if marker_pos != -1 else 0\n",
        "    \n",
        "    for step in steps:\n",
        "        start = full_text.find(step['text'])\n",
        "        if start == -1:\n",
        "            step['importance'] = 0.0\n",
        "            continue\n",
        "        \n",
        "        prefix_toks = len(tokenizer.encode(full_text[:start], add_special_tokens=False))\n",
        "        step_toks = len(tokenizer.encode(step['text'], add_special_tokens=False))\n",
        "        \n",
        "        t_start = max(0, min(offset + prefix_toks, len(last_attn) - 1))\n",
        "        t_end = max(t_start + 1, min(offset + prefix_toks + step_toks, len(last_attn)))\n",
        "        \n",
        "        step['importance'] = float(np.sum(last_attn[t_start:t_end]))\n",
        "        step['num_tokens'] = step_toks\n",
        "    \n",
        "    # Normalize\n",
        "    total = sum(s['importance'] for s in steps)\n",
        "    if total > 0:\n",
        "        for s in steps:\n",
        "            s['importance'] /= total\n",
        "    \n",
        "    return steps\n",
        "\n",
        "\n",
        "def prune_steps(steps, thinking, answer, threshold):\n",
        "    \"\"\"Remove steps below the threshold percentile of importance.\"\"\"\n",
        "    if threshold == 0.0:\n",
        "        return thinking + '\\n' + answer, len(steps), len(steps)\n",
        "    \n",
        "    importances = [s['importance'] for s in steps]\n",
        "    cutoff = np.percentile(importances, threshold * 100)\n",
        "    \n",
        "    kept = [s for s in steps if s['importance'] >= cutoff]\n",
        "    if not kept:\n",
        "        kept = [max(steps, key=lambda s: s['importance'])]\n",
        "    \n",
        "    pruned_text = '\\n\\n'.join(s['text'] for s in kept) + '\\n' + answer\n",
        "    return pruned_text, len(steps), len(kept)\n",
        "\n",
        "\n",
        "def extract_answer(text):\n",
        "    \"\"\"Extract numeric answer (AIME: 0-999).\"\"\"\n",
        "    for pattern in [r'\\\\boxed\\{(\\d+)\\}', r'the answer is\\s*[:\\s]*(\\d+)', \n",
        "                     r'final answer[:\\s]*(\\d+)', r'= (\\d+)\\s*$']:\n",
        "        matches = re.findall(pattern, text, re.IGNORECASE | re.MULTILINE)\n",
        "        if matches:\n",
        "            try:\n",
        "                ans = int(matches[-1])\n",
        "                if 0 <= ans <= 999:\n",
        "                    return ans\n",
        "            except ValueError:\n",
        "                continue\n",
        "    \n",
        "    numbers = re.findall(r'\\b(\\d{1,3})\\b', text[-200:])\n",
        "    return int(numbers[-1]) if numbers else None\n",
        "\n",
        "\n",
        "def reevaluate(problem, pruned_reasoning):\n",
        "    \"\"\"Re-evaluate with pruned context.\"\"\"\n",
        "    prompt = f'Problem: {problem}\\n\\nReasoning:\\n{pruned_reasoning}\\n\\nBased on the above, the final answer is:'\n",
        "    return generate_reasoning(prompt, max_tokens=512)\n",
        "\n",
        "print('Functions defined.')"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Run Experiment"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "THRESHOLDS = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5]\n",
        "all_results = []\n",
        "metrics = {t: {'correct': 0, 'total': 0, 'kept_pct': [], 'lengths': []} for t in THRESHOLDS}\n",
        "\n",
        "for i, prob in enumerate(AIME24_PROBLEMS):\n",
        "    print(f'\\n{\"=\"*50}')\n",
        "    print(f'Problem {i+1}/{len(AIME24_PROBLEMS)}')\n",
        "    print(f'{\"=\"*50}')\n",
        "    \n",
        "    # Generate full reasoning\n",
        "    t0 = time.time()\n",
        "    full_text = generate_reasoning(prob['problem'])\n",
        "    print(f'  Generated {len(full_text)} chars in {time.time()-t0:.1f}s')\n",
        "    \n",
        "    # Segment & score\n",
        "    steps, thinking, answer = segment_steps(full_text)\n",
        "    print(f'  {len(steps)} reasoning steps found')\n",
        "    \n",
        "    if not steps:\n",
        "        for t in THRESHOLDS:\n",
        "            metrics[t]['total'] += 1\n",
        "        continue\n",
        "    \n",
        "    try:\n",
        "        steps = score_steps_by_attention(steps, full_text, prob['problem'])\n",
        "    except Exception as e:\n",
        "        print(f'  Attention scoring failed: {e}')\n",
        "        for s in steps:\n",
        "            s['importance'] = 1.0 / len(steps)\n",
        "    \n",
        "    # Evaluate at each threshold\n",
        "    prob_result = {'id': prob['id'], 'expected': prob['answer']}\n",
        "    \n",
        "    for t in THRESHOLDS:\n",
        "        pruned, orig, kept = prune_steps(steps, thinking, answer, t)\n",
        "        \n",
        "        if t == 0.0:\n",
        "            predicted = extract_answer(full_text)\n",
        "        else:\n",
        "            eval_out = reevaluate(prob['problem'], pruned)\n",
        "            predicted = extract_answer(eval_out)\n",
        "            if predicted is None:\n",
        "                predicted = extract_answer(pruned)\n",
        "        \n",
        "        correct = predicted is not None and predicted == prob['answer']\n",
        "        \n",
        "        metrics[t]['correct'] += int(correct)\n",
        "        metrics[t]['total'] += 1\n",
        "        metrics[t]['kept_pct'].append(kept / max(orig, 1))\n",
        "        metrics[t]['lengths'].append(len(pruned))\n",
        "        \n",
        "        print(f'  t={t:.1f}: {orig}->{kept} steps, pred={predicted}, exp={prob[\"answer\"]}, {\"✓\" if correct else \"✗\"}')\n",
        "    \n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "print('\\nExperiment complete!')"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Results & Comparison Table"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "print('\\n' + '=' * 80)\n",
        "print('COMPARISON TABLE: Attention-Based Reasoning Pruning on AIME24')\n",
        "print(f'Model: {MODEL_NAME}')\n",
        "print('=' * 80)\n",
        "print(f'{\"Threshold\":<12} {\"Accuracy\":<12} {\"Steps Kept\":<14} {\"Avg Length\":<14} {\"Correct/Total\"}')\n",
        "print('-' * 80)\n",
        "\n",
        "baseline_acc = None\n",
        "baseline_len = None\n",
        "\n",
        "for t in THRESHOLDS:\n",
        "    m = metrics[t]\n",
        "    if m['total'] == 0:\n",
        "        continue\n",
        "    acc = m['correct'] / m['total'] * 100\n",
        "    kept = np.mean(m['kept_pct']) * 100 if m['kept_pct'] else 0\n",
        "    avg_len = np.mean(m['lengths']) if m['lengths'] else 0\n",
        "    \n",
        "    if t == 0.0:\n",
        "        baseline_acc = acc\n",
        "        baseline_len = avg_len\n",
        "        label = f'{t:.1f} (base)'\n",
        "    else:\n",
        "        label = f'{t:.1f}'\n",
        "    \n",
        "    delta_acc = f' ({acc - baseline_acc:+.1f})' if baseline_acc is not None and t > 0 else ''\n",
        "    delta_len = f' (-{(1 - avg_len/baseline_len)*100:.0f}%)' if baseline_len and t > 0 else ''\n",
        "    \n",
        "    print(f'{label:<12} {acc:.1f}%{delta_acc:<8} {kept:.1f}%{\"\":<9} '\n",
        "          f'{avg_len:.0f}{delta_len:<9} {m[\"correct\"]}/{m[\"total\"]}')\n",
        "\n",
        "print('=' * 80)\n",
        "print()\n",
        "print('Key Findings:')\n",
        "print('- Threshold 0.0 = full reasoning chain (baseline)')\n",
        "print('- Moderate pruning (0.1-0.3) removes distracting steps,')\n",
        "print('  often maintaining or improving accuracy (\"Think Clearly\" effect)')\n",
        "print('- Aggressive pruning (0.4+) risks removing critical reasoning steps')"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Attention Heatmap Visualization"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Visualize attention importance for the last problem\n",
        "if steps:\n",
        "    fig, ax = plt.subplots(1, 1, figsize=(12, 4))\n",
        "    \n",
        "    step_ids = [s['step_id'] for s in steps]\n",
        "    importances = [s['importance'] for s in steps]\n",
        "    \n",
        "    colors = ['#2ecc71' if imp > np.median(importances) else '#e74c3c' for imp in importances]\n",
        "    \n",
        "    ax.bar(step_ids, importances, color=colors, alpha=0.8)\n",
        "    ax.set_xlabel('Reasoning Step ID')\n",
        "    ax.set_ylabel('Attention Importance Score')\n",
        "    ax.set_title('Attention-Based Step Importance\\n(Green=Important, Red=Prunable)')\n",
        "    ax.axhline(y=np.median(importances), color='black', linestyle='--', alpha=0.5, label='Median')\n",
        "    ax.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Save Results"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "results_summary = {\n",
        "    'model': MODEL_NAME,\n",
        "    'dataset': 'AIME24',\n",
        "    'num_problems': len(AIME24_PROBLEMS),\n",
        "    'thresholds': {}\n",
        "}\n",
        "\n",
        "for t in THRESHOLDS:\n",
        "    m = metrics[t]\n",
        "    if m['total'] > 0:\n",
        "        results_summary['thresholds'][str(t)] = {\n",
        "            'accuracy': m['correct'] / m['total'],\n",
        "            'avg_steps_kept_pct': float(np.mean(m['kept_pct'])) * 100 if m['kept_pct'] else 0,\n",
        "            'avg_output_length': float(np.mean(m['lengths'])) if m['lengths'] else 0,\n",
        "            'correct': m['correct'],\n",
        "            'total': m['total'],\n",
        "        }\n",
        "\n",
        "with open('aime24_pruning_results.json', 'w') as f:\n",
        "    json.dump(results_summary, f, indent=2)\n",
        "\n",
        "print('Results saved to aime24_pruning_results.json')\n",
        "print('Upload this notebook and results to GitHub!')"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    }
  ]
}
